{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import plot\n",
    "import torch\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('clip_model_v2.pt')\n",
    "model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = torch.load('clip_model_v2.pt')\n",
    "model.model.eval()\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "frame_count = 0\n",
    "\n",
    "predicted_label = \"dadada\"\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Capture frame-by-frame\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Process every 30th frame or so for analysis; adjust the frequency as needed\n",
    "    # if frame_count % 30 == 0:\n",
    "    #     predicted_label = model.predict(frame)\n",
    "    #     cv2.putText(frame, {predicted_label},(50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    # predicted_label = model.predict(frame)\n",
    "    # print(predicted_label)\n",
    "\n",
    "    predicted_label = model.predict(frame)['label']\n",
    "    cv2.putText(frame, predicted_label,(50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "    frame_count += 1\n",
    "    # Break the loop with the 'q' key\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n",
      "ALERT: fire in office detected consistently. Take action!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load your custom model (make sure it's in evaluation mode if necessary)\n",
    "model = torch.load('clip_model_v2.pt')\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Prediction settings\n",
    "prediction_threshold = 0.23\n",
    "danger_labels = set(['fight on a street', 'fire on a street', 'street violence','car crash','fire in office'])  # Add more as needed\n",
    "\n",
    "# For alerting\n",
    "alert_count_threshold = 60  # Number of consecutive frames a danger label must be detected to trigger an alert\n",
    "current_alert_label = None\n",
    "current_alert_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Assuming model.predict returns a dictionary with 'label' and 'confidence'\n",
    "    prediction = model.predict(frame)  # This needs to be defined to work with your model\n",
    "    label, confidence = prediction['label'], prediction['confidence']\n",
    "\n",
    "    # Check if the prediction is above the threshold and is a danger label\n",
    "    if confidence >= prediction_threshold and label in danger_labels:\n",
    "        if label == current_alert_label:\n",
    "            current_alert_count += 1\n",
    "        else:\n",
    "            current_alert_label = label\n",
    "            current_alert_count = 1\n",
    "\n",
    "        if current_alert_count >= alert_count_threshold:\n",
    "            print(f\"ALERT: {label} detected consistently. Take action!\")\n",
    "            # Additional alerting logic can be added here (e.g., sending notifications)\n",
    "    else:\n",
    "        current_alert_label = None\n",
    "        current_alert_count = 0\n",
    "\n",
    "    # Optionally display the prediction on the frame\n",
    "    cv2.putText(frame, f\"{label}: {confidence:.2f}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Assuming model loading and initialization as before\n",
    "model = torch.load('clip_model_v2.pt')\n",
    "model.model.eval()\n",
    "\n",
    "danger_labels = set(['fight on a street', 'fire on a street', 'street violence','car crash','fire in office','violence in office','fire','violence','accident'])\n",
    "\n",
    "label_mapping = {\n",
    "    'fire on a street': 'fire',\n",
    "    'fire in office': 'fire',\n",
    "    'fight on a street': 'violence',\n",
    "    'street violence': 'violence',\n",
    "    'car crash': 'accident',\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "frame_window_size = 25  # Number of frames in a 3-second window at 30 fps\n",
    "alert_threshold = 19  # Minimum number of frames with the same class to trigger an alert\n",
    "recent_predictions = []  # Initialize a list to keep track of recent predictions\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Directly using model.predict() as indicated\n",
    "    predicted_label = model.predict(frame)['label']  # Replace this with your actual model's prediction method if necessary\n",
    "\n",
    "    # Add the predicted label to the recent predictions list\n",
    "    if predicted_label in  danger_labels:\n",
    "        predicted_label = label_mapping.get(predicted_label, predicted_label)\n",
    "    recent_predictions.append(predicted_label)\n",
    "\n",
    "    if len(recent_predictions) > frame_window_size:\n",
    "        recent_predictions.pop(0)  \n",
    "\n",
    "    # Check if any label meets the alert criteria\n",
    "    if len(recent_predictions) >= frame_window_size:\n",
    "        most_common_label = max(set(recent_predictions), key=recent_predictions.count)\n",
    "        count_most_common = recent_predictions.count(most_common_label)\n",
    "\n",
    "        if count_most_common >= alert_threshold and most_common_label in danger_labels:\n",
    "            print(f\"ALERT: {most_common_label} detected consistently in the last 3 seconds.\")\n",
    "            # Option to clear the list for reset after alerting\n",
    "            # recent_predictions.clear()\n",
    "\n",
    "    cv2.putText(frame, f\"{predicted_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video Feed', frame)\n",
    "\n",
    "    # Break out of the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Assuming model loading and initialization as before\n",
    "model = torch.load('clip_model_v2.pt')\n",
    "model.model.eval()\n",
    "\n",
    "danger_labels = set(['fight on a street', 'fire on a street', 'street violence', 'car crash', 'fire in office', 'violence in office', 'fire', 'violence', 'accident'])\n",
    "\n",
    "label_mapping = {\n",
    "    'fire on a street': 'fire',\n",
    "    'fire in office': 'fire',\n",
    "    'fight on a street': 'violence',\n",
    "    'street violence': 'violence',\n",
    "    'car crash': 'accident',\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "frame_window_size = 25  # Number of frames in a 3-second window at 30 fps\n",
    "alert_threshold = 19  # Minimum number of frames with the same class to trigger an alert\n",
    "cooldown_period = 5 * 60  # 5 minutes cooldown period in seconds\n",
    "alert_display_duration = 3  # Duration to display alert message in seconds\n",
    "\n",
    "recent_predictions = []  # Initialize a list to keep track of recent predictions\n",
    "last_alert_time = 0  # Time of the last alert\n",
    "display_alert_until = 0  # Time until which to display the alert message\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not frame:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    predicted_label = model.predict(frame)['label']  # Replace this with your actual model's prediction method if necessary\n",
    "\n",
    "    if predicted_label in danger_labels:\n",
    "        predicted_label = label_mapping.get(predicted_label, predicted_label)\n",
    "    recent_predictions.append(predicted_label)\n",
    "\n",
    "    if len(recent_predictions) > frame_window_size:\n",
    "        recent_predictions.pop(0)\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    if len(recent_predictions) >= frame_window_size and (current_time - last_alert_time) > cooldown_period:\n",
    "        most_common_label = max(set(recent_predictions), key=recent_predictions.count)\n",
    "        count_most_common = recent_predictions.count(most_common_label)\n",
    "\n",
    "        if count_most_common >= alert_threshold and most_common_label in danger_labels:\n",
    "            print(f\"ALERT: {most_common_label} detected consistently in the last 3 seconds.\")\n",
    "            last_alert_time = current_time\n",
    "            display_alert_until = current_time + alert_display_duration\n",
    "            # Clear recent_predictions to reset detection after alerting\n",
    "            recent_predictions.clear()\n",
    "\n",
    "    # Display the predicted label or alert message\n",
    "    if current_time <= display_alert_until:\n",
    "        # Display alert message in red text\n",
    "        cv2.putText(frame, f\"{most_common_label} Alert\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        # Display the current prediction in green text\n",
    "        cv2.putText(frame, f\"{predicted_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video Feed', frame)\n",
    "\n",
    "    # Break out of the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<firebase_admin.App at 0x2d0f8ee3b88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from firebase_admin import credentials\n",
    "import firebase_admin\n",
    "\n",
    "cred = credentials.Certificate('./serviceAccountKey.json')\n",
    "firebase_admin.initialize_app(cred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firebase alert\n",
    "from io import BytesIO\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from firebase_admin import db\n",
    "from firebase_admin import storage, firestore\n",
    "import smtplib\n",
    "\n",
    "\n",
    "# def upload_frame_to_firebase(frame, file_path):\n",
    "#     # Convert the frame to JPEG format\n",
    "#     _, buffer = cv2.imencode('.jpg', frame)\n",
    "#     image_data = BytesIO(buffer)\n",
    "    \n",
    "#     # Create a reference to the upload path in Firebase Storage\n",
    "#     bucket = storage.bucket('smart-threat-detector.appspot.com')\n",
    "#     blob = bucket.blob(file_path)\n",
    "    \n",
    "#     # Upload the image\n",
    "#     blob.upload_from_file(image_data, content_type='image/jpeg')\n",
    "    \n",
    "#     # Make the blob publicly viewable\n",
    "#     blob.make_public()\n",
    "    \n",
    "#     print(f\"Uploaded frame to {file_path}\")\n",
    "#     return blob.public_url\n",
    "\n",
    "# def add_alert_to_database(image_url, location):\n",
    "#     # Get the current time in UTC\n",
    "#     utc_now = datetime.now(pytz.UTC)\n",
    "    \n",
    "#     # Format the data to write to the database\n",
    "#     data = {\n",
    "#         'image_url': image_url,\n",
    "#         'location': location,\n",
    "#         'time': utc_now.isoformat()\n",
    "#     }\n",
    "    \n",
    "#     # Create a unique key for the new alert entry\n",
    "#     ref = db.reference('/alerts')\n",
    "#     new_alert_ref = ref.push()\n",
    "    \n",
    "#     # Write the data to the new alert entry\n",
    "#     new_alert_ref.set(data)\n",
    "    \n",
    "#     print(f\"Alert added to database with URL: {image_url}\")\n",
    "\n",
    "#define threat squad for Fire, Accident, Violence in a map \n",
    "threat_squad = {\n",
    "    'fire': ['Fire Station', 'Police', 'Medical'],\n",
    "    'accident': ['Police', 'Medical'],\n",
    "    'violence': ['Police']\n",
    "}\n",
    "\n",
    "def add_alert_to_database(file_path,frame, location,alert_type):\n",
    "    # Convert the frame to JPEG format\n",
    "    _, buffer = cv2.imencode('.jpg', frame)\n",
    "    image_data = BytesIO(buffer)\n",
    "    \n",
    "    # Create a reference to the upload path in Firebase Storage\n",
    "    bucket = storage.bucket('godsend-aef97.appspot.com')\n",
    "    blob = bucket.blob(file_path)\n",
    "    \n",
    "    # Upload the image\n",
    "    blob.upload_from_file(image_data, content_type='image/jpeg')\n",
    "    \n",
    "    # Make the blob publicly viewable\n",
    "    blob.make_public()\n",
    "    \n",
    "    print(f\"Uploaded frame to {file_path}\")\n",
    "\n",
    "    image_url = blob.public_url\n",
    "    # Get the current time in UTC\n",
    "    utc_now = datetime.now(pytz.UTC)\n",
    "    \n",
    "    # Format the data to write to the database\n",
    "    data = {\n",
    "        'cordinates': firestore.GeoPoint(37.422, -122.084),\n",
    "        'address': 'NMAMIT, Nitte, Karkala, Karnataka, India',\n",
    "        \"image_link\": [image_url],\n",
    "        'squads': threat_squad[alert_type],\n",
    "        \"threat\": alert_type,\n",
    "        'timeStamp': utc_now.isoformat()\n",
    "    }\n",
    "\n",
    "    db = firestore.client()\n",
    "\n",
    "    # Add a new document\n",
    "    db.collection(location).document().set(data)\n",
    "\n",
    "#create a mail send notification which fetch admin detail from database \"sqaud_users\" have a field \"email\" notify all the admin of category in \"category\" field defines the threat_squad\n",
    "# async def send_mail_notification(location,alert_type):\n",
    "#     db = firestore.client()\n",
    "#     # Get all the admin emails for the corresponding category of squad\n",
    "#     admin_emails = db.collection('squad_users').where('category', 'in', threat_squad[alert_type]).stream()\n",
    "    \n",
    "#     for admin in admin_emails:\n",
    "#         email = admin.to_dict()['email']\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#create alert function to add in database and send mail alert make \n",
    "async def create_alert(frame, location,alert_type):\n",
    "    # Save the frame to Firebase Storage\n",
    "    file_path = f\"frames/{location}/alert_{datetime.now().isoformat()}.jpg\"\n",
    "    await add_alert_to_database(file_path,frame, location,alert_type)\n",
    "\n",
    "    # Send email notifications to the corresponding squad members\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Additional alerting logic can be added here (e.g., sending notifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: fire detected consistently in the last 3 seconds.\n",
      "Uploaded frame to alerts/fire_1711094780.271099.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "\n",
    "# Assuming model loading and initialization as before\n",
    "model = torch.load('clip_model_v2.pt')\n",
    "model.model.eval()\n",
    "\n",
    "danger_labels = set(['fight on a street', 'fire on a street', 'street violence', 'car crash', 'fire in office', 'violence in office', 'fire', 'violence', 'accident'])\n",
    "\n",
    "label_mapping = {\n",
    "    'fire on a street': 'fire',\n",
    "    'fire in office': 'fire',\n",
    "    'fight on a street': 'violence',\n",
    "    'street violence': 'violence',\n",
    "    'car crash': 'accident',\n",
    "}\n",
    "\n",
    "video_path = './fire_cctv.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "frame_window_size = 25  # Number of frames in a 3-second window at 30 fps\n",
    "alert_threshold = 19  # Minimum number of frames with the same class to trigger an alert\n",
    "cooldown_period = 5 * 60  # 5 minutes cooldown period in seconds\n",
    "alert_display_duration = 3  # Duration to display alert message in seconds\n",
    "\n",
    "recent_predictions = []  # Initialize a list to keep track of recent predictions\n",
    "last_alert_time = 0  # Time of the last alert\n",
    "display_alert_until = 0  # Time until which to display the alert message\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    predicted_label = model.predict(frame)['label']  # Replace this with your actual model's prediction method if necessary\n",
    "\n",
    "    if predicted_label in danger_labels:\n",
    "        predicted_label = label_mapping.get(predicted_label, predicted_label)\n",
    "    recent_predictions.append(predicted_label)\n",
    "\n",
    "    if len(recent_predictions) > frame_window_size:\n",
    "        recent_predictions.pop(0)\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    if len(recent_predictions) >= frame_window_size and (current_time - last_alert_time) > cooldown_period:\n",
    "        most_common_label = max(set(recent_predictions), key=recent_predictions.count)\n",
    "        count_most_common = recent_predictions.count(most_common_label)\n",
    "\n",
    "        if count_most_common >= alert_threshold and most_common_label in danger_labels:\n",
    "            print(f\"ALERT: {most_common_label} detected consistently in the last 3 seconds.\")\n",
    "            last_alert_time = current_time\n",
    "            display_alert_until = current_time + alert_display_duration\n",
    "            # Clear recent_predictions to reset detection after alerting\n",
    "            recent_predictions.clear()\n",
    "\n",
    "    # Display the predicted label or alert message\n",
    "    if current_time <= display_alert_until:\n",
    "        # Display alert message in red text\n",
    "        cv2.putText(frame, f\"{most_common_label} Alert\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        timestamp = datetime.now().timestamp()\n",
    "        file_path = f\"alerts/{most_common_label}_{timestamp}.jpg\"\n",
    "        add_alert_to_database(file_path,frame, \"Karkala\",most_common_label)\n",
    "    else:\n",
    "        # Display the current prediction in green text\n",
    "        if predicted_label in danger_labels:\n",
    "            cv2.putText(frame, f\"{predicted_label}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Normal\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "    cv2.imshow('Video Feed', frame)\n",
    "\n",
    "    # Break out of the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
